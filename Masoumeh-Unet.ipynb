{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "## My configuration/path\n",
    "data_path = \"/home/mas/Desktop/Project-ai-master/data/GlaS/\"\n",
    "grade_file = \"Grade.csv\"\n",
    "\n",
    "csv_file=data_path+grade_file\n",
    "\n",
    "pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Alexander Hustinx\n",
    "# Date: 8-06-2018\n",
    "#\n",
    "# GlaS Dataset\n",
    "# Version: v0.1\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "## My configuration/path\n",
    "data_path = \"/home/mas/Desktop/Project-ai-master/data/GlaS/\"\n",
    "grade_file = \"Grade.csv\"\n",
    "\n",
    "class GlaSDataset(Dataset):\n",
    "    \"\"\" GlaS Dataset  \"\"\"\n",
    "    def __init__(self, csv_file=data_path+grade_file, root_dir=data_path, transform=None, transform_anno=None, desired_dataset=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file: path to the grade csv-file\n",
    "            root_dir: path to the map containing the images\n",
    "            transform: (optional) transformation to be applied on sample['image']\n",
    "            transform_anno: (optional) transformation to be applied on the sample['image_anno']\n",
    "            desired_dataset: (optional) rows where the name does not contains this keyword will be deleted\n",
    "                    this allows you to split the dataset into 'train' and 'test'\n",
    "        \"\"\"\n",
    "        print(csv_file)\n",
    "        # File extensions *cough* hardcoded *cough*\n",
    "        self.image_ext = '.bmp'\n",
    "        self.annotation_label = '_anno'\n",
    "        \n",
    "        print(root_dir)\n",
    "        print(csv_file)\n",
    "        #Load csv-file into pandas\n",
    "        self.framework = pd.read_csv(csv_file)\n",
    "        \n",
    "        #Get rid of those pesky whitespaces at the start and end of the grades\n",
    "        self.framework[' grade (GlaS)'] = self.framework[' grade (GlaS)'].str.strip()\n",
    "        self.framework[' grade (Sirinukunwattana et al. 2015)'] = self.framework[' grade (Sirinukunwattana et al. 2015)'].str.strip()\n",
    "        \n",
    "        #Remove all rows not containing the given desired_dataset, allowing to split 'test' and 'train'\n",
    "        if desired_dataset:\n",
    "            self.framework = self.framework[self.framework['name'].str.contains(desired_dataset) == True]\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.transform_anno = transform_anno\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.framework)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Sample format:\n",
    "            image: image containing the to segment/grade cells\n",
    "            image_anno: image containing the segmented cells\n",
    "            patient_id: id of the patient the cell originated from\n",
    "            GlaS: assigned GlaS grade (target #1)\n",
    "            grade: assigned (Sirinukunwattana et al. 2015) grade (target #2)\n",
    "        \"\"\"\n",
    "        \n",
    "        image_name = self.root_dir + self.framework.iloc[index, 0]\n",
    "        image = io.imread(image_name + self.image_ext)\n",
    "        image_anno = io.imread(image_name + self.annotation_label + self.image_ext)\n",
    "        \n",
    "        #Currently unused, but future-proofing\n",
    "        patient_id = self.framework.iloc[index, 1]\n",
    "        \n",
    "        GlaS = self.framework.iloc[index, 2]\n",
    "        grade = self.framework.iloc[index, 3]\n",
    "        \n",
    "        sample = {'image':image, 'image_anno':image_anno, 'patient_id':patient_id, 'GlaS':GlaS, 'grade':grade}\n",
    "        \n",
    "        #Currently unused, but future-proofing (This will be the supplied preprocessing/data augmentation)\n",
    "        if self.transform:\n",
    "            #PIL-image must be HxWxC, thus must have 3 dimensions\n",
    "            if len(sample['image_anno'].shape) == 2:\n",
    "                sample['image_anno'] = np.expand_dims(sample['image_anno'], axis=2)\n",
    "            sample['image'] = transforms.functional.to_pil_image(sample['image'])\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        \n",
    "        if self.transform_anno:\n",
    "            #PIL-image must be HxWxC, thus must have 3 dimensions\n",
    "            if len(sample['image_anno'].shape) == 2:\n",
    "                sample['image_anno'] = np.expand_dims(sample['image_anno'], axis=2)\n",
    "            sample['image_anno'] = transforms.functional.to_pil_image(sample['image_anno'])\n",
    "            sample['image_anno'] = self.transform_anno(sample['image_anno'])\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "## Example for the proof-of-concept:\n",
    "##         Draws the first 4 images and their segmentations\n",
    "##        Including their GlaS grade and (Sirinukunwattana et al. 2015) grade\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #load dataset\n",
    "    fig = plt.figure()\n",
    "    dataset = GlaSDataset(desired_dataset='test')\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        #load a sample\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        print(\"Index #{}:\\n\\tPatient id:\\t\\t{}\\n\\tImage size:\\t\\t{}\\n\\tAnnotated image size:\\t{}\\n\\tGlaS grade:\\t\\t{}\\n\\tOther grade:\\t\\t{}\"\n",
    "            .format(i, sample['patient_id'], sample['image'].shape, sample['image_anno'].shape, sample['GlaS'], sample['grade']))\n",
    "        \n",
    "        ##plots: start\n",
    "        ax = plt.subplot(2, 4, i + 1)\n",
    "        plt.tight_layout()\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Sample #{}'.format(i))\n",
    "        plt.imshow(sample['image'])\n",
    "        \n",
    "        ax = plt.subplot(2, 4, i + 5)\n",
    "        plt.tight_layout()\n",
    "        ax.axis('off')\n",
    "        plt.imshow(sample['image_anno'])\n",
    "        ##plots: end\n",
    "\n",
    "        #we only show 3, proof-of-concept\n",
    "        if i == 3:\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Alexander Hustinx\n",
    "# Date: 12-06-2018\n",
    "#\n",
    "# GlaS DataLoader example and transformation example\n",
    "# Version: v1.1\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "\n",
    "from GlaS_dataset import GlaSDataset\n",
    "\n",
    "data_path = \"/home/mas/Desktop/Project-ai-master/data/GlaS/\"\n",
    "grade_file = \"Grade.csv\"\n",
    "\n",
    "def imshow(input, title=None):    \n",
    "    images_batch = input['image']\n",
    "    \n",
    "    print('images_batch.shape: ', images_batch.shape)\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images_batch)\n",
    "    \n",
    "    print('grid.shape: ', grid.shape)\n",
    "    print('grid T . shape: ', grid.numpy().transpose((1, 2, 0)).shape)\n",
    "    \n",
    "    plt.imshow(grid.numpy().transpose((1,2,0)))\n",
    "    plt.title('batch from dataloader')\n",
    "        \n",
    "## Example for the proof-of-concept:\n",
    "##         Draws the first 4 images and their segmentations\n",
    "##        Including their GlaS grade and (Sirinukunwattana et al. 2015) grade\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 4\n",
    "    \n",
    "    data_transform = transforms.Compose([transforms.Scale((256,256)),transforms.ToTensor()])\n",
    "    \n",
    "    \n",
    "    print(data_transform)\n",
    "    #load train dataset\n",
    "    GlaS_train_dataset = GlaSDataset(csv_file=data_path+grade_file, root_dir=data_path,transform=data_transform,\n",
    "                                transform_anno=data_transform, \n",
    "                                desired_dataset='train')\n",
    "\n",
    "    #load test dataset\n",
    "    GlaS_test_dataset = GlaSDataset(csv_file=data_path+grade_file, root_dir=data_path,transform=data_transform,\n",
    "                                transform_anno=data_transform,\n",
    "                                desired_dataset='test')\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(GlaS_train_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=1)\n",
    "                            \n",
    "    test_loader = DataLoader(GlaS_test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=1)\n",
    "                            \n",
    "    #loop over the set\n",
    "    for batch_i, sampled_batch in enumerate(train_loader):\n",
    "        print(\"Index #{}:\\n\\tPatient id:\\t\\t{}\\n\\tImage size:\\t\\t{}\\n\\tAnnotated image size:\\t{}\\n\\tGlaS grade:\\t\\t{}\\n\\tOther grade:\\t\\t{}\"\n",
    "            .format(batch_i, sampled_batch['patient_id'], sampled_batch['image'].shape, sampled_batch['image_anno'].shape, sampled_batch['GlaS'], sampled_batch['grade']))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Observe the 3rd batch\n",
    "        if batch_i == 2:\n",
    "            plt.figure()\n",
    "            imshow(sampled_batch)\n",
    "            plt.axis('off')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            ##plots: end\n",
    "            \n",
    "            break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I assume that we did image augmentation now I have an normalized and cropped image and mask.\n",
    "# I assume that have also hyper params files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "{'filterNumStart': 'int', 'lr': 'listfloat', 'epochs': 'int', 'lambda2': 'float', 'batchSize': 'int', 'doBatchNorm': 'int', 'channels': 'liststring', 'dropout': 'float', 'depth': 'int', 'mask': 'string', 'labels': 'string', 'dataPath': 'string'}\n",
      "['filterNumStart', '64']\n",
      "['lr', '0.0001,0.0003']\n",
      "['epochs', '20']\n",
      "['lambda2', '0.0001']\n",
      "['batchSize', '32']\n",
      "['doBatchNorm', '1']\n",
      "['dropout', '0.3']\n",
      "['depth', '5']\n",
      "1\n",
      "522 775 3\n",
      "----------- Building Encoder -------------\n",
      "3 64\n",
      "64 64\n",
      "64 128\n",
      "128 128\n",
      "128 256\n",
      "256 256\n",
      "256 512\n",
      "512 512\n",
      "512 1024\n",
      "1024 1024\n",
      "1024 1024\n",
      "----------- Building Decoder -------------\n",
      "1024 512\n",
      "1024 512\n",
      "512 512\n",
      "512 256\n",
      "512 256\n",
      "256 256\n",
      "256 128\n",
      "256 128\n",
      "128 128\n",
      "128 64\n",
      "128 64\n",
      "64 64\n",
      "torch.Size([1, 512, 64, 64])\n",
      "torch.Size([1, 512, 56, 56])\n",
      "torch.Size([1, 256, 136, 136])\n",
      "torch.Size([1, 256, 104, 104])\n",
      "torch.Size([1, 128, 280, 280])\n",
      "torch.Size([1, 128, 200, 200])\n",
      "torch.Size([1, 64, 568, 568])\n",
      "torch.Size([1, 64, 392, 392])\n"
     ]
    }
   ],
   "source": [
    "# U-net\n",
    "# Project AI\n",
    "# 13 June 2018, Masoumeh \n",
    "\n",
    "# Initial dummy version\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import scipy.ndimage\n",
    "\n",
    "\n",
    "# ---------------------------------- helper functions -------------------------------------------\n",
    "def load_dataset():\n",
    "    # ------------------------\n",
    "    # TODO:    \n",
    "    train = defaultdict(list) # train[\"image\"][i] = i th image of training set\n",
    "    validation = defaultdict(list)\n",
    "    # ------------------------    \n",
    "    \n",
    "    return train,validation\n",
    "\n",
    "\n",
    "def load_hyperparams(param_path):    \n",
    "    param_file = open(param_path,\"r\")\n",
    "    \n",
    "    hyperparams = defaultdict()\n",
    "\n",
    "    keywords = ['filterNumStart',\"lr\", \"epochs\", \"lambda2\", \"batchSize\", \"doBatchNorm\", \"channels\", \"dropout\",\"depth\", \"mask\", \"labels\", \"dataPath\"]\n",
    "    types = [\"int\",\"listfloat\", \"int\", \"float\", \"int\", \"int\", \"liststring\", \"float\",\"int\",\"string\", \"string\", \"string\"]\n",
    "    \n",
    "    \n",
    "    key_type = {}\n",
    "    for i in range(len(keywords)):\n",
    "        key_type[keywords[i]] = types[i]        \n",
    "    print(key_type)\n",
    "    \n",
    "    for line in param_file:\n",
    "        info = line.replace(' ','').strip().split('=')\n",
    "        #print(info)\n",
    "        #print('----------------')\n",
    "        list_value = []\n",
    "        if(key_type[info[0]] in [\"int\", \"listfloat\", \"float\"]):\n",
    "            print(info)            \n",
    "            hyperparams[info[0]] = list(map(float,info[1].split(',')))\n",
    "        else:\n",
    "            hyperparams[info[0]] = info[1].split(',')\n",
    "        \n",
    "    return hyperparams\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(encoder, self).__init__()\n",
    "   \n",
    "        \n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        \n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, channels, hyper_params):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        # later will change these to loop over all options for every epoch\n",
    "        self.network_depth = int(hyper_params['depth'][0])\n",
    "        filter_num = int(hyper_params['filterNumStart'][0])\n",
    "        in_channels = int(channels)\n",
    "            \n",
    "        \n",
    "        print(\"----------- Building Encoder -------------\")\n",
    "        self.down_blocks = []\n",
    "        self.up_blocks = []\n",
    "        print(in_channels, filter_num)\n",
    "        for d in range(self.network_depth):\n",
    "            block_d = {}\n",
    "\n",
    "            block_d['conv1'] = nn.Sequential(nn.Conv2d(in_channels, filter_num, kernel_size = 3, stride=1, padding=0), nn.BatchNorm2d(filter_num), nn.ReLU())\n",
    "            in_channels = filter_num            \n",
    "            print(in_channels, filter_num)\n",
    "            block_d['conv2'] = nn.Sequential(nn.Conv2d(in_channels, filter_num, kernel_size = 3, stride=1, padding=0), nn.BatchNorm2d(filter_num), nn.ReLU())\n",
    "            if(d != self.network_depth-1):\n",
    "                block_d['maxpool'] = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "                filter_num = filter_num * 2\n",
    "            self.down_blocks.append(block_d)\n",
    "            print(in_channels, filter_num)\n",
    "           \n",
    "        print(\"----------- Building Decoder -------------\")    \n",
    "        in_channels = filter_num\n",
    "        for d in range(int(self.network_depth - 1)):\n",
    "            block_u = {}\n",
    "            filter_num = int(in_channels / 2)\n",
    "            print(in_channels, filter_num)\n",
    "            block_u['upconv'] = nn.Sequential(nn.ConvTranspose2d(in_channels, filter_num, kernel_size=2, stride=2), nn.BatchNorm2d(filter_num), nn.ReLU())            \n",
    "            \n",
    "            print(in_channels, filter_num)\n",
    "            block_u['conv1'] = nn.Sequential(nn.Conv2d(in_channels, filter_num, kernel_size = 3, stride=1, padding=0), nn.BatchNorm2d(filter_num), nn.ReLU())\n",
    "            \n",
    "            in_channels = int(in_channels / 2)\n",
    "            print(in_channels, filter_num)\n",
    "            block_u['conv2'] = nn.Sequential(nn.Conv2d(in_channels, filter_num, kernel_size = 3, stride=1, padding=0), nn.BatchNorm2d(filter_num), nn.ReLU())\n",
    "            \n",
    "            self.up_blocks.append(block_u)\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n)) \n",
    "                    \n",
    "    def forward(self,x):\n",
    "        # torch.nn.functional.leaky_relu_(input, negative_slope=0.01)\n",
    "        concat_features = []\n",
    "        for d in range(self.network_depth):\n",
    "            x = self.down_blocks[d]['conv1'](x)\n",
    "            x = self.down_blocks[d]['conv2'](x)\n",
    "            concat_features.append(x)\n",
    "            if d != self.network_depth - 1:\n",
    "                x = self.down_blocks[d]['maxpool'](x)\n",
    "        # now create the upsampling path\n",
    "        \n",
    "        for d in range(self.network_depth-1):\n",
    "            x = self.up_blocks[d]['upconv'](x)\n",
    "\n",
    "            #torch.concat([concat_features[self.network_depth-2-d], x], 1) # TODO: crop the concat feature\n",
    "            x = self.crop_and_concat(x, concat_features[self.network_depth-2-d], crop=True)\n",
    "\n",
    "            x = self.up_blocks[d]['conv1'](x)\n",
    "            x = self.up_blocks[d]['conv2'](x)\n",
    "        \n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "    # from https://discuss.pytorch.org/t/cropping-images-in-a-batch-on-the-gpu/7485/2\n",
    "    def crop_and_concat(self, upsampled, bypass, crop=False):\n",
    "        if crop:\n",
    "            print(bypass.size())\n",
    "            print(upsampled.size())\n",
    "            c = (bypass.size()[2] - upsampled.size()[2]) // 2  # assumes equal width/height\n",
    "            bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "        \n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    augmented_mask = \"/home/mas/Desktop/Project-ai-master/data/GlaS/testA_1_anno.bmp\"\n",
    "    augmented_image = \"/home/mas/Desktop/Project-ai-master/data/GlaS/testA_1.bmp\"\n",
    "    \n",
    "    hyper_params = load_hyperparams(\"/home/mas/Desktop/Project-ai-master/data/hyper_params\")\n",
    "    \n",
    "    print(len(hyper_params['filterNumStart']))\n",
    "    \n",
    "    \n",
    "    \n",
    "    height, width, channels = scipy.ndimage.imread(augmented_image).shape    \n",
    "    print(height, width, channels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    net = UNet(channels,hyper_params)\n",
    "    x = Variable(torch.FloatTensor(np.random.random((1,3,572, 572))))\n",
    "    out = net(x)\n",
    "    #main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
